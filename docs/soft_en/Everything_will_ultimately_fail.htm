<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
<meta charset="UTF-8">
<link rel="stylesheet" href="../css/style_en.css"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<title>Everything will ultimately fail - Software Architect 97Things</title>
<meta property="og:title" content="Everything will ultimately fail">
<meta property="og:type" content="article">
<meta property="og:image" content="https://yoshi389111.github.io/kinokobooks/images/soft97_en.png">
<meta property="og:url" content="https://yoshi389111.github.io/kinokobooks/soft_en/Everything_will_ultimately_fail.htm">
<meta property="og:site_name" content="97 Things Every Software Architect Should Know.">
<meta property="og:locale" content="en_US">

</head>
<body>
<article>
<header>
<h1>Everything will ultimately fail</h1>
</header>

<p>Hardware is fallible, so we add redundancy. This allows us to
survive individual hardware failures, but increases the likelihood of
having at least one failure present at any given time.
</p><p>Software is fallible. Our applications are made of software, so
they're vulnerable to failures. We add monitoring to tell us when the
applications fail, but that monitoring is made of more software, so it
too is fallible.
</p><p>Humans make mistakes; we are fallible also. So, we automate
actions, diagnostics, and processes. Automation removes the chance for
an error of comission, but increases the chance of an error of
omission. No automated system can respond to the same range of
situations that a human can.
</p><p>Therefore, we add monitoring to the automation. More software, more opportunities for failures.
</p><p>Networks are built out of hardware, software, and very long
wires. Therefore, networks are fallible. Even when they work, they are
unpredictable because the state space of a large network is, for all
practical purposes, infinite. Individual components may act
deterministically, but still produce essentially chaotic behavior.
</p><p>Every safety mechanism we employ to mitigate one kind of
failure adds new failure modes. We add clustering software to move
applications from a failed server to a healthy one, but now we risk
"split-brain syndrome" if the cluster's network acts up.
</p><p>It's worth remembering that the Three Mile Island accident was
largely caused by a pressure relief value---a safety mechanism meant to
prevent certain types of overpressure failures.
</p><p>So, faced with the certainty of failure in our systems, what can we do about it?
</p><p>Accept that, no matter what, your system will have a variety of
failure modes. Deny that inevitability, and you lose your power to
control and contain them. Once you accept that failures will happen,
you have the ability to design your system's reaction to specific
failures. Just as auto engineers create crumple zones---areas designed
to protect passengers by failing first---you can create safe failure
modes that contain the damage and protect the rest of the system. 
If you do not design your failure modes, then you will get
whatever unpredictable---and usually dangerous---ones happen to emerge.
</p>

<footer>
<p class="author">By Michael Nygard</p>
<p class="license">This work is licensed under a <a href="https://creativecommons.org/licenses/by/3.0/us/" class="external text" title="https://creativecommons.org/licenses/by/3.0/us/" rel="nofollow">Creative Commons Attribution 3</a></p>

</footer>
</article>
</body>
</html>

